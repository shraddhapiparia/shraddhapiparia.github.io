<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>What Regularization Means in Representation Learning | Shraddha Piparia</title>

  <link rel="stylesheet" href="../style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>

<body>
  <header class="header">
    <nav class="nav">
      <a class="logo" href="../index.html">SP</a>
      <button class="nav__toggle" aria-label="Toggle menu">&#9776;</button>
      <ul class="nav__menu">
        <li><a href="../propub.html">Select Publications</a></li>
        <li><a href="../awards.html">Awards</a></li>
        <li><a href="../bookshelf.html">Book&nbsp;Shelf</a></li>
        <li><a href="../articles.html" aria-current="page">Articles</a></li>
      </ul>
    </nav>
  </header>

  <main class="pagebox">
    <article class="post">
      <header class="post__header">
        <h1>What Regularization Means in Representation Learning</h1>
      </header>

      <p>
        Regularization is often described as a way to prevent overfitting. While this description is broadly accurate, it can obscure an important distinction: different regularization methods constrain different parts of a model.
      </p>

      <p>
        In representation learning, particularly in variational autoencoders (VAEs), this distinction becomes more visible. Regularization influences not only generalization performance, but also what kinds of latent representations the model is encouraged to learn.
      </p>

      <p>
        This post introduces two broad categories of regularization and focuses on distribution-level regularization commonly used in VAEs, with emphasis on KL divergence and Maximum Mean Discrepancy (MMD). It does not aim to describe the full geometry of latent spaces, which is discussed separately.
      </p>

      <h2>Two broad levels of regularization</h2>

      <p>
        Regularization methods can be grouped according to what they constrain. Some act directly on model parameters. Others act on distributions implied by the model.
      </p>

      <p>
        Both influence what is learned, but they do so through different mechanisms and should not be treated as interchangeable.
      </p>

      <h2>Parameter-level regularization</h2>

      <p>
        Parameter-level methods act directly on network weights or activations. They primarily constrain the complexity of the learned function.
      </p>

      <p>
        Common examples include:
      </p>

      <ul>
        <li>L2 regularization (weight decay), which discourages large weights.</li>
        <li>L1 regularization, which encourages sparse parameter usage.</li>
        <li>Dropout, which introduces stochasticity during training.</li>
      </ul>

      <p>
        These techniques can improve robustness and reduce overfitting, but they do not explicitly specify how a latent space should be organized.
      </p>

      <h2>Distribution-level regularization</h2>

      <p>
        Distribution-level regularization constrains the distribution of latent variables rather than individual parameters. This becomes central in VAEs, where the encoder defines a distribution over latent variables for each input.
      </p>

      <p>
        In a VAE, training balances reconstruction accuracy with a constraint that encourages the encoder-defined latent distributions to match a chosen prior. This constraint is not only about generalization. It is a modeling choice about what kinds of representations are allowed.
      </p>

      <h2>Why VAEs introduce distribution-level constraints</h2>

      <p>
        Reconstruction objectives alone do not specify how latent space should behave globally. A model can achieve good reconstruction while learning a latent space that is difficult to compare or reuse across samples.
      </p>

      <p>
        Distribution-level regularization introduces a global reference by encouraging latent representations to follow a shared prior. The goal is not to remove information, but to shape how information is organized.
      </p>

      <h2>KL divergence in VAEs</h2>

      <p>
        KL divergence measures how much the encoder’s approximate posterior deviates from the prior distribution. In the standard VAE formulation, it limits how far the model can push latent representations away from the prior in order to improve reconstruction.
      </p>

      <p>
        In practice, KL regularization:
      </p>

      <ul>
        <li>Encourages alignment between posterior and prior distributions.</li>
        <li>Promotes a latent space that is easier to sample from and compare across inputs.</li>
        <li>Is typically weighted to balance reconstruction and regularization.</li>
      </ul>

      <p>
        If KL regularization is overly strong, the latent variables may carry little information about the input. This behavior is often referred to as posterior collapse. Whether it occurs depends on architecture, loss scaling, and training dynamics.
      </p>

      <h2>MMD as an alternative</h2>

      <p>
        Maximum Mean Discrepancy offers an alternative way to compare distributions by matching samples using kernel-based statistics. Instead of computing an analytic divergence, it compares samples drawn from the model’s aggregated posterior to samples drawn from the prior.
      </p>

      <p>
        Compared to KL divergence, MMD:
      </p>

      <ul>
        <li>Operates at the sample level rather than requiring a closed-form divergence.</li>
        <li>Can be more flexible in how distribution mismatch is measured.</li>
        <li>Introduces sensitivity to kernel choice and batch-level effects.</li>
      </ul>

      <p>
        MMD-based VAEs are often discussed in settings where KL-based training produces overly constrained or unstable representations, though they also introduce additional design choices.
      </p>

      <h2>Choosing between KL and MMD</h2>

      <p>
        The choice between KL divergence and MMD reflects different assumptions about how latent distributions should be constrained. Each method shapes the latent distribution in a different way and introduces different sensitivities during training.
      </p>

      <p>
        This choice is therefore not only an optimization detail. It is part of defining what kind of representation the model is encouraged to learn.
      </p>

      <h2>Implications for genomic representation learning</h2>

      <p>
        In applications such as genotype-based embeddings, reconstruction error alone may not reflect whether a representation is useful. Representations are often evaluated by their stability across runs, their comparability across samples, and their suitability for downstream analysis.
      </p>

      <p>
        Distribution-level regularization influences these properties by controlling how latent variation is distributed and how strongly representations are tied to a shared reference prior.
      </p>

      <h2>Takeaway</h2>

      <p>
        Regularization encompasses multiple mechanisms operating at different levels of a model. Parameter-level methods constrain model complexity. Distribution-level methods constrain latent distributions and therefore influence representation learning directly.
      </p>

      <p>
        In VAEs, KL divergence and MMD are two common ways of imposing distribution-level constraints. Understanding what they constrain is a prerequisite for understanding how latent space structure changes under different regularization choices.
      </p>

      <p>
        In the next article, we focus on that latent space behavior and discuss how regularization shapes continuity, interpretability, and downstream usefulness of learned embeddings.
      </p>

    </article>
  </main>

  <script>
    const navToggle = document.querySelector('.nav__toggle');
    const navMenu  = document.querySelector('.nav__menu');
    if (navToggle && navMenu) {
      navToggle.addEventListener('click', () => navMenu.classList.toggle('nav__menu--open'));
    }
  </script>
</body>
</html>
