<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>What Regularization Means in Representation Learning | Shraddha Piparia</title>

  <link rel="stylesheet" href="../style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>

<body>
  <header class="header">
    <nav class="nav">
      <a class="logo" href="../index.html">SP</a>
      <button class="nav__toggle" aria-label="Toggle menu">&#9776;</button>
      <ul class="nav__menu">
        <li><a href="../propub.html">Select Publications</a></li>
        <li><a href="../awards.html">Awards</a></li>
        <li><a href="../bookshelf.html">Book&nbsp;Shelf</a></li>
        <li><a href="../articles.html" aria-current="page">Articles</a></li>
      </ul>
    </nav>
  </header>

  <main class="pagebox">
    <article class="post">
      <h1>What Regularization Means in Representation Learning</h1>

      <p>
        Regularization is often described as a way to prevent overfitting. While this description is broadly accurate, it can obscure an important distinction: different regularization methods act on different aspects of a model.
      </p>

      <p>
        In representation learning, particularly in variational autoencoders (VAEs), this distinction becomes more visible. Regularization influences not only generalization performance, but also the properties of the learned latent representation.
      </p>

      <p>
        This post focuses on regularization methods commonly used in VAEs, with emphasis on KL divergence and Maximum Mean Discrepancy (MMD), and briefly situates them alongside more familiar techniques.
      </p>

      <h2>Two broad levels of regularization</h2>

      <p>
        Regularization methods can be loosely grouped according to what they constrain.
      </p>

      <h2>Parameter-level regularization</h2>

      <p>
        Parameter-level methods act directly on network weights or activations.
      </p>

      <p>
        Common examples include:
      </p>

      <ul>
        <li>L2 regularization (weight decay), which discourages large weights.</li>
        <li>L1 regularization, which encourages sparse parameter usage.</li>
        <li>Dropout, which introduces stochasticity during training.</li>
      </ul>

      <p>
        These techniques primarily influence the complexity of the learned function. They do not explicitly constrain the structure of a learned latent representation.
      </p>

      <h2>Distribution-level regularization</h2>

      <p>
        Distribution-level regularization constrains the distribution of latent variables rather than individual parameters.
      </p>

      <p>
        This becomes relevant in VAEs, where the model is trained to match an encoder-defined distribution to a chosen prior. KL divergence and MMD are two commonly used approaches in this setting.
      </p>

      <h2>Why VAEs introduce distribution-level constraints</h2>

      <p>
        VAEs balance reconstruction accuracy with a constraint on the latent distribution. Without this constraint, the latent space can become highly structured in ways that improve reconstruction but make comparison or reuse of embeddings more difficult.
      </p>

      <p>
        Distribution-level regularization is often used to encourage continuity, comparability, or stability in the latent space, depending on the modeling goal.
      </p>

      <h2>KL divergence in VAEs</h2>

      <p>
        KL divergence measures how much the encoderâ€™s approximate posterior deviates from the prior distribution. In the standard VAE formulation, it limits how much information the latent variables carry beyond what is needed for reconstruction.
      </p>

      <p>
        In practice, KL regularization:
      </p>

      <ul>
        <li>Encourages alignment between posterior and prior distributions.</li>
        <li>Promotes smoother latent representations.</li>
        <li>Is typically weighted to balance reconstruction and regularization.</li>
      </ul>

      <p>
        If overly strong, KL regularization can reduce the information carried by the latent variables, a behavior often referred to as posterior collapse. This outcome depends on architecture, loss scaling, and training dynamics.
      </p>

      <h2>MMD as an alternative</h2>

      <p>
        Maximum Mean Discrepancy offers an alternative way to compare distributions by matching samples using kernel-based statistics.
      </p>

      <p>
        Compared to KL divergence, MMD:
      </p>

      <ul>
        <li>Does not rely on an explicit parametric form for the distributions.</li>
        <li>Operates at the sample level rather than through an analytic divergence.</li>
        <li>Introduces sensitivity to kernel choice and batch-level effects.</li>
      </ul>

      <p>
        In some settings, MMD can provide a more flexible form of distribution matching, though it also introduces additional design choices.
      </p>

      <h2>Choosing between KL and MMD</h2>

      <p>
        The choice between KL divergence and MMD is typically guided by modeling priorities rather than reconstruction error alone.
      </p>

      <ul>
        <li>KL divergence is often chosen for its simplicity and theoretical grounding.</li>
        <li>MMD may be preferred when KL-based training leads to unstable or overly constrained representations.</li>
      </ul>

      <p>
        Switching between these regularizers changes how the latent space is shaped and how embeddings should be interpreted.
      </p>

      <h2>Implications for genomic representation learning</h2>

      <p>
        In applications such as genotype or LD-aware embeddings, reconstruction metrics alone may not reflect whether a representation is useful.
      </p>

      <p>
        Properties such as stability across runs, comparability across genomic blocks, and suitability for downstream modeling are influenced by distribution-level regularization choices.
      </p>

      <h2>Takeaway</h2>

      <p>
        Regularization encompasses multiple mechanisms operating at different levels of a model.
      </p>

      <p>
        In VAEs used for representation learning, KL divergence and MMD provide different ways of shaping latent distributions. Choosing between them is part of defining what kind of representation a model is encouraged to learn.
      </p>

    </article>
  </main>

  <script>
    const navToggle = document.querySelector('.nav__toggle');
    const navMenu  = document.querySelector('.nav__menu');
    if (navToggle && navMenu) {
      navToggle.addEventListener('click', () => navMenu.classList.toggle('nav__menu--open'));
    }
  </script>
</body>
</html>
