<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Regularization Isn’t One Thing | Shraddha Piparia</title>

  <link rel="stylesheet" href="../style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>

<body>
  <header class="header">
    <nav class="nav">
      <a class="logo" href="../index.html">SP</a>
      <button class="nav__toggle" aria-label="Toggle menu">&#9776;</button>
      <ul class="nav__menu">
        <li><a href="../propub.html">Select Publications</a></li>
        <li><a href="../awards.html">Awards</a></li>
        <li><a href="../bookshelf.html">Book&nbsp;Shelf</a></li>
        <li><a href="../articles.html" aria-current="page">Articles</a></li>
      </ul>
    </nav>
  </header>

  <main class="pagebox">
    <article class="post">
      <h1>Regularization Isn’t One Thing</h1>

      <p>Regularization is often described as a general tool to prevent overfitting. While this is true, it hides an important distinction: different regularization methods constrain different parts of a model. In representation learning, especially with variational autoencoders (VAEs), this distinction matters because regularization does not only affect generalization. It shapes the latent space itself.</p>

      <p>This post focuses on regularization methods relevant to VAEs, particularly KL divergence and Maximum Mean Discrepancy (MMD), and briefly situates them among more familiar techniques.</p>

      <h2>Two categories of regularization</h2>
      <p>Regularization methods can be broadly grouped into two categories.</p>

      <h2>Parameter-level regularization</h2>
      <p>These methods act directly on model parameters or activations.</p>

      <p>Common examples include:</p>
      <ul>
        <li>L2 regularization (weight decay), which discourages large weights and promotes smoother mappings.</li>
        <li>L1 regularization, which encourages sparse parameter usage.</li>
        <li>Dropout, which introduces stochasticity during training to reduce reliance on specific activations.</li>
      </ul>

      <p>These techniques are widely used and effective, but they primarily regularize the function learned by the network, not the structure of a learned representation.</p>

      <h2>Distribution-level regularization</h2>
      <p>Distribution-level regularization constrains the distribution of latent variables rather than the network parameters themselves.</p>

      <p>This category becomes important in VAEs, where the model explicitly learns a latent variable distribution. KL divergence and MMD belong to this group.</p>

      <h2>Why VAEs require distribution-level regularization</h2>
      <p>A VAE optimizes two competing objectives: accurate reconstruction of inputs, and a latent distribution that remains close to a chosen prior.</p>

      <p>Without the second constraint, the latent space can become arbitrarily structured in ways that improve reconstruction but reduce interpretability, continuity, or reusability. Distribution-level regularization enforces global structure on the latent space, making it suitable for downstream tasks such as clustering, similarity search, or transformer-based modeling.</p>

      <h2>KL divergence in VAEs</h2>
      <p>KL divergence measures how much the encoder’s approximate posterior deviates from the prior distribution. In the standard VAE formulation, this term explicitly penalizes information gain beyond what is necessary for reconstruction.</p>

      <p>Key properties of KL regularization:</p>
      <ul>
        <li>It enforces a well-defined relationship between posterior and prior distributions.</li>
        <li>It encourages smooth, continuous latent spaces.</li>
        <li>Its strength is controlled by a weighting factor (often referred to as beta in beta-VAEs).</li>
      </ul>

      <p>However, KL regularization can dominate training if not carefully balanced, leading to posterior collapse where latent variables carry little information. This behavior depends on architecture, reconstruction scale, and training dynamics.</p>

      <h2>MMD as an alternative to KL</h2>
      <p>Maximum Mean Discrepancy provides an alternative way to regularize latent distributions. Instead of computing a parametric divergence, MMD compares samples from the encoder distribution and the prior using kernel-based statistics.</p>

      <p>Important characteristics of MMD:</p>
      <ul>
        <li>It does not require explicit parametric assumptions about the distributions being compared.</li>
        <li>It matches distributions at the sample level rather than through an analytic expression.</li>
        <li>Its behavior depends on kernel choice and bandwidth, which introduces additional design considerations.</li>
      </ul>

      <p>In practice, MMD can offer more flexible distribution matching but may be sensitive to scaling and batch-level effects.</p>

      <h2>Choosing between KL and MMD</h2>
      <p>The choice between KL and MMD depends on the modeling goal rather than reconstruction performance alone.</p>

      <ul>
        <li>KL divergence is often preferred for its simplicity, interpretability, and strong theoretical grounding.</li>
        <li>MMD may be useful when KL leads to unstable training or overly restrictive latent representations.</li>
      </ul>

      <p>These regularizers are not interchangeable. Each defines a different notion of latent space health, and switching between them changes the semantics of the learned embeddings.</p>

      <h2>Implications for representation learning in genomics</h2>
      <p>In applications such as genotype or LD-aware embeddings, reconstruction metrics alone are insufficient. The utility of embeddings depends on latent space structure: stability across blocks, comparability across runs, and suitability for downstream modeling.</p>

      <p>Distribution-level regularization directly controls these properties. For this reason, KL or MMD should be viewed as part of the representation design, not merely as optimization details.</p>

      <h2>Takeaway</h2>
      <p>Regularization is not a single concept. Parameter-level methods control model complexity, while distribution-level methods shape latent representations.</p>

      <p>For VAEs used in representation learning, KL divergence and MMD play a central role by defining the structure and usability of the latent space. Choosing between them is a modeling decision that affects not only training behavior, but also what the embeddings ultimately represent.</p>
    </article>
  </main>

  <script>
    const navToggle = document.querySelector('.nav__toggle');
    const navMenu  = document.querySelector('.nav__menu');
    if (navToggle && navMenu) {
      navToggle.addEventListener('click', () => navMenu.classList.toggle('nav__menu--open'));
    }
  </script>
</body>
</html>
