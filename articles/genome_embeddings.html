<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Designing LD-Aware Genome Embeddings: Modeling choices and Trade-offs | Shraddha Piparia</title>

  <!-- site-wide stylesheet and fonts -->
  <link rel="stylesheet" href="../style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">
</head>

<body>
  <!-- ========= HEADER / NAV ========= -->
  <header class="header">
    <nav class="nav">
      <a class="logo" href="../index.html">SP</a>
      <button class="nav__toggle" aria-label="Toggle menu">&#9776;</button>
      <ul class="nav__menu">
        <li><a href="../propub.html">Projects</a></li>
        <li><a href="../awards.html">Awards</a></li>
        <li><a href="../bookshelf.html">Book&nbsp;Shelf</a></li>
        <li><a href="../articles.html" aria-current="page">Articles</a></li>
      </ul>
    </nav>
  </header>

  <!-- ========= ARTICLE ========= -->
  <main class="pagebox">
    <article class="post">
      <header class="post__header">
        <h1>Designing LD-Aware Genome Embeddings: Modeling choices and Trade-offs</h1>
        <p class="post__meta">
          Published&nbsp;<time datetime="2025-09-09">25&nbsp;December&nbsp;2025</time>
        </p>
      </header>

      <p>
        The scale of genotype data is intimidating. Millions of variants, thousands of individuals, and matrices that grow wider far faster than they grow tall. A natural temptation is to treat this as a representation learning problem: reduce dimensionality, learn embeddings, and let downstream models discover structure.
      </p>

      <p>
        But in genetics, how you compress the data matters as much as how much you compress it. The genome is not a bag of independent features. Its structure is biological, inherited, and unevenly distributed. Embeddings that ignore this structure may be mathematically compact while remaining misaligned with biological organization. This post is not about proposing a single “best” model. It lays out design constraints that shape LD-aware genome embeddings, and the trade-offs that follow from them.
      </p>

      <h2>Why SNP-level representations are often insufficient</h2>

      <p>
        At first glance, SNP data looks well suited to modern machine learning: discrete inputs, large feature space, and clear downstream objectives. In practice, SNP-level representations often become unstable as dimensionality grows.
      </p>

      <p>
        The fundamental issue is dimensionality. Even modest genotyping arrays contain hundreds of thousands of variants, while cohort sizes are typically in the thousands. Treating each SNP as an independent feature creates an extreme imbalance that encourages overfitting and unstable representations.
      </p>

      <p>
        More subtly, SNPs are not independent. Many neighboring variants are highly correlated due to shared inheritance. A model that treats them as separate inputs is forced to relearn the same signal repeatedly, which can dilute meaningful variation and amplify redundancy. Increasing model capacity does not necessarily solve this and can make optimization more fragile.
      </p>

      <h2>Linkage disequilibrium as a modeling constraint</h2>

      <p>
        Linkage disequilibrium (LD) is often introduced as a technical complication: something to correct for, prune away, or adjust out. From a representation learning perspective, LD can also be understood as a structural prior.
      </p>

      <p>
        LD reflects which variants tend to be inherited together. It encodes cis-level dependencies shaped by recombination history, population structure, and local genomic architecture. Ignoring LD effectively treats nearby variants as exchangeable within the model.
      </p>

      <p>
        This has direct consequences for embeddings. If correlated SNPs are embedded independently, the resulting latent space can reflect redundancy rather than distinct biological variation. Grouping correlated variants encourages the model to respect local units of variation, at least within the limits of the chosen grouping scheme.
      </p>

      <h2>LD pruning as a practical simplification</h2>

      <p>
        One common response to linkage disequilibrium is pruning: removing correlated variants until the remaining set is approximately independent. This strategy is widespread because it directly addresses the dimensionality problem. By reducing redundancy, LD pruning produces genotype matrices that are easier to store, model, and interpret.
      </p>

      <p>
        From a modeling perspective, LD pruning makes an explicit trade-off. It preserves broad patterns of variation while discarding much of the local correlation structure. For tasks focused on association testing or population-level summaries, this can be acceptable and sometimes desirable.
      </p>

      <p>
        However, pruning changes the question being asked. Instead of modeling how variants co-vary within genomic neighborhoods, the model sees a sparse representation in which local structure has already been collapsed. This simplification often stabilizes learning, but it also limits what the representation can express about cis-level organization.
      </p>

      <p>
        LD is often treated as redundancy to be reduced, but it also encodes information about how the genome is organized.
      </p>

      <h2>Why embeddings are used at all</h2>

      <p>
        Classic linear methods like PCA succeed precisely because they exploit correlation structure. They compress variance efficiently and reveal broad population patterns. But PCA emphasizes global axes of variation, often aligned with ancestry, which can make more localized effects harder to represent.
      </p>

      <p>
        Many questions in medical and functional genomics sit in a middle ground: beyond raw SNPs, but below genome-wide averages. Researchers often want representations that are compact and continuous enough to support downstream modeling, while still retaining information that is relevant at local genomic scales.
      </p>

      <p>
        This is where learned embeddings become attractive. Unlike fixed linear projections, embeddings can adapt to non-linear structure and distribute information across dimensions. The challenge is ensuring that what they learn aligns with biology rather than artifacts of preprocessing or cohort composition.
      </p>

      <h2>LD blocks as a practical compromise</h2>

      <p>
        Organizing variants into LD blocks is an imperfect but practical response to these constraints. Blocks impose locality, reduce redundancy, and define units that are large enough to capture cis structure while small enough to remain computationally manageable.
      </p>

      <p>
        No block definition is “correct” in an absolute sense. Different thresholds reflect different assumptions about how much correlation is meaningful. Smaller blocks preserve resolution but risk fragmentation; larger blocks capture broader structure but blur distinct signals. Choosing a block scheme is not only a preprocessing step; it is also a modeling decision.
      </p>

      <p>
        Blocks are a compromise. They respect local structure but deliberately ignore interactions beyond their boundaries. Any downstream model needs to account for what is lost at that interface.
      </p>

      <h2>Why a VAE, not just another compressor</h2>

      <p>
        Once blocks are defined, the next question is how to represent them. Linear compression methods are fast and interpretable, but they assume that variation lies along straight axes. Genomic variation is not always well captured by linear structure alone.
      </p>

      <p>
        Variational autoencoders (VAEs) offer a different perspective. By learning a continuous latent space with explicit regularization, VAEs can encourage smoothness: nearby genotypes map to nearby representations. This shifts emphasis away from reconstruction accuracy toward properties of the latent space such as continuity, stability, and comparability across samples.
      </p>

      <p>
        Regularization choices matter here. Strong constraints can improve stability and comparability but may oversimplify local variation. Weaker constraints can preserve detail but risk fragmentation or run-to-run drift. These are not hyperparameters to tune blindly; they encode assumptions about what kinds of variation should be emphasized.
      </p>

      <h2>Scaling beyond local structure</h2>

      <p>
        LD-aware block embeddings address cis-level dependencies, but they typically do not represent long-range or trans interactions. Yet many phenotypes emerge from coordinated effects across the genome.
      </p>

      <p>
        Scaling up therefore becomes a conceptual problem, not only a computational one. How should local embeddings be combined? In what order? With what inductive biases? Treating block embeddings as independent tokens risks ignoring interactions that may be relevant for downstream phenotypes.
      </p>

      <p>
        Second-stage models—often attention-based—are a natural place to reintroduce broader context. Here, attention operates over compressed genomic units rather than raw sequence. The goal is not to learn genomic grammar, but to model relationships between learned representations.
      </p>

      <h2>A brief contrast with sequence language models</h2>

      <p>
        It is tempting to ask why these problems are not addressed directly by genomic language models. Sequence models can learn local rules from contiguous DNA such as motifs and regulatory grammar. They operate on a single reference genome and are often less exposed to population-level confounding.
      </p>

      <p>
        Genotype data answers a different question. It reflects how sequence-level rules are instantiated across individuals, shaped by LD, ancestry, and polygenic architecture. Representations learned from genotype matrices need to absorb population context rather than abstract away from it.
      </p>

      <p>
        In this sense, LD-aware embeddings are not alternatives to sequence language models. They address a downstream problem that begins where sequence grammar is already partly characterized.
      </p>

      <h2>Interpretability constrains which models are worth building, even if interpretation is applied later</h2>

      <p>
        Interpretability in genotype embeddings is often framed as attribution: which variants, blocks, or regions drive a particular prediction. While important, attribution is usually not the first problem to solve.
      </p>

      <p>
        For representation learning, a more immediate concern is whether the embedding is stable and meaningfully grounded. Latent spaces that drift across runs, collapse onto population axes, or change dramatically with minor preprocessing choices are difficult to interpret regardless of which attribution method is applied later.
      </p>

      <h2>What LD-aware embeddings can make possible</h2>

      <p>
        LD-aware genome embeddings introduce their own assumptions and limitations, particularly when LD structure is specific to a given population.
      </p>

      <p>
        Within a defined ancestry, such embeddings can make it easier to model polygenic structure without being trivially dominated by population axes or redundant local correlations. By compressing correlated variants into locally grounded units, they can yield representations that are more stable and less redundant than raw SNP matrices while remaining computationally tractable.
      </p>

      <p>
        This framework can support downstream analyses that depend on intermediate representations, such as exploring polygenic architecture, relating genetic structure to phenotypes, or assessing sensitivity to modeling choices. Extending these ideas beyond a single population remains a separate challenge, closely tied to how LD varies across cohorts.
      </p>

    </article>
  </main>

  <script>
    const navToggle = document.querySelector('.nav__toggle');
    const navMenu  = document.querySelector('.nav__menu');
    navToggle.addEventListener('click', () => navMenu.classList.toggle('nav__menu--open'));
  </script>
</body>
</html>
