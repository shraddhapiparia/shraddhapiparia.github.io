<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>KL vs MMD in VAEs: Two Ways to Regularize a Latent Space | Shraddha Piparia</title>

  <link rel="stylesheet" href="../style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;700&family=Inter:wght@300;400;600&display=swap" rel="stylesheet">

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['\\(','\\)'], ['$', '$']] },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <!-- Small patch: prevent long equations from breaking page width -->
  <style>
    mjx-container[jax="CHTML"][display="true"] {
      overflow-x: auto;
      overflow-y: hidden;
      max-width: 100%;
      padding-bottom: 0.25rem;
    }
  </style>
</head>

<body>
  <header class="header">
    <nav class="nav">
      <a class="logo" href="../index.html">SP</a>
      <button class="nav__toggle" aria-label="Toggle menu">&#9776;</button>
      <ul class="nav__menu">
        <li><a href="../propub.html">Projects</a></li>
        <li><a href="../awards.html">Awards</a></li>
        <li><a href="../bookshelf.html">Book&nbsp;Shelf</a></li>
        <li><a href="../articles.html" aria-current="page">Articles</a></li>
      </ul>
    </nav>
  </header>

  <main class="pagebox">
    <article class="post">
      <header class="post__header">
        <h1>KL vs MMD in VAEs: Two Ways to Regularize a Latent Space</h1>
        <p class="post__meta">Feb 5, 2026</p>
      </header>

      <p>
        Variational autoencoders (VAEs) are often introduced as “autoencoders with a probabilistic latent space.”
        In practice, the defining feature is simpler: VAEs learn an encoder distribution and then
        <em>force that distribution to behave</em> so the model remains generative.
      </p>

      <p>
        That “force” typically appears as a divergence term. Most VAEs use KL divergence because it drops out of the ELBO.
        But KL is not the only principled way to align distributions—especially if we want flexibility in the shape of the latent space.
        This post lays out the ELBO connection cleanly, then contrasts KL divergence with MMD as two different
        distribution-matching strategies.
      </p>

      <h2>From marginal likelihood to the ELBO</h2>

      <p>
        Start with a latent variable model with joint distribution \(p_\theta(x,z)\). The marginal likelihood is
      </p>

      \[
        \log p_\theta(x) = \log \int p_\theta(x,z)\,dz.
      \]

      <p>
        The posterior \(p_\theta(z\mid x)\) is generally intractable, so we introduce a variational approximation \(q_\phi(z\mid x)\)
        and rewrite:
      </p>

      \[
        \log p_\theta(x)
        = \log \int q_\phi(z\mid x)\,\frac{p_\theta(x,z)}{q_\phi(z\mid x)}\,dz.
      \]

      <p>
        Applying Jensen’s inequality yields a lower bound:
      </p>

      \[
        \log p_\theta(x)
        \ge
        \mathbb{E}_{q_\phi(z\mid x)}\left[\log p_\theta(x,z) - \log q_\phi(z\mid x)\right],
      \]

      <p>
        which is the <strong>Evidence Lower Bound (ELBO)</strong>. Expanding the joint as \(p_\theta(x,z)=p_\theta(x\mid z)p(z)\):
      </p>

      \[
        \mathcal{L}(\theta,\phi;x)
        =
        \mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
        -
        \mathrm{KL}\big(q_\phi(z\mid x)\,\|\,p(z)\big).
      \]

      <p>
        The ELBO separates learning into:
        (1) a reconstruction term that preserves information about \(x\) in \(z\), and
        (2) a regularization term that aligns the encoder distribution to a chosen prior.
        Importantly, the KL term isn’t an arbitrary penalty—it emerges from the variational formulation.
      </p>

      <h2>What KL divergence enforces</h2>

      \[
        \mathrm{KL}(q_\phi(z\mid x)\|p(z))=
        \mathbb{E}_{q_\phi(z\mid x)}[\log q_\phi(z\mid x)-\log p(z)].
      \]

      <p>
        KL compares <em>explicit densities</em>. In practice this usually means strong distributional assumptions
        (e.g., \(q_\phi(z\mid x)\) is diagonal Gaussian and \(p(z)=\mathcal{N}(0,I)\)) so the KL term is tractable
        and stable to optimize.
      </p>

      <p>
        Those assumptions also shape the geometry of the learned latent space: each per-sample posterior \(q(z\mid x)\)
        is pressured to resemble the same unimodal isotropic prior. With expressive decoders, this can lead to
        under-utilized latent dimensions or posterior collapse.
      </p>

      <h2>MMD: distribution matching through samples</h2>

      <p>
        <strong>Maximum Mean Discrepancy (MMD)</strong> provides an alternative: instead of comparing densities,
        it compares distributions through their samples using a kernel-defined notion of similarity.
      </p>

      <p>
        A key shift is that MMD is often used to match the <strong>aggregated posterior</strong>
        \[
          q(z) = \mathbb{E}_{x}[q_\phi(z\mid x)]
        \]
        to the prior \(p(z)\), rather than enforcing a strict per-sample match for every \(x\).
      </p>

      \[
        \mathrm{MMD}^2(q,p) =
        \mathbb{E}_{z,z'\sim q}[k(z,z')]
        + \mathbb{E}_{z,z'\sim p}[k(z,z')]
        - 2\,\mathbb{E}_{z\sim q,\,z'\sim p}[k(z,z')].
      \]

      <p>
        A common choice is the radial basis function (RBF) kernel:
      </p>

      \[
        k(x,y)=\exp\left(-\frac{\|x-y\|^2}{2\sigma^2}\right).
      \]

      <h2>ELBO with MMD regularization</h2>

      \[
        \mathcal{L}_{\text{MMD}}
        =
        \mathbb{E}_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
        -
        \lambda\,\mathrm{MMD}(q(z),p(z)).
      \]

      <p>
        The most useful mental model for me is:
        <em>KL enforces where probability mass should be</em>,
        while <em>MMD enforces how samples are arranged</em>.
      </p>

    </article>
  </main>

  <script>
    const navToggle = document.querySelector('.nav__toggle');
    const navMenu  = document.querySelector('.nav__menu');
    navToggle.addEventListener('click', () => navMenu.classList.toggle('nav__menu--open'));
  </script>
</body>
</html>
